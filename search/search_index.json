{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLM Intermediate Representation","text":"<p>Consistently typed intermediate representation for LLM chats in Python</p> \ud83d\udcd6 API Reference \ud83d\udd17 GitHub"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install llmir\n</code></pre> <p>Requires Python 3.11</p>"},{"location":"api/chunks/","title":"Chunks","text":""},{"location":"api/chunks/#src.llmir.chunks.AIChunkText","title":"<code>AIChunkText</code>","text":"<p>               Bases: <code>RichReprMixin</code>, <code>BaseModel</code></p> <p>A text chunk in the LLM conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['text']</code> <p>The type of the chunk, only for discriminated unions.</p> <code>text</code> <code>str</code> <p>The text content.</p> Source code in <code>src/llmir/chunks.py</code> <pre><code>class AIChunkText(RichReprMixin, BaseModel):\n    \"\"\"\n    A text chunk in the LLM conversation.\n\n    Attributes:\n        type: The type of the chunk, only for discriminated unions.\n        text: The text content.\n    \"\"\"\n\n    type: Literal[\"text\"] = \"text\"\n    text: str\n</code></pre>"},{"location":"api/chunks/#src.llmir.chunks.AIChunkFile","title":"<code>AIChunkFile</code>","text":"<p>               Bases: <code>RichReprMixin</code>, <code>BaseModel</code></p> <p>A file chunk in the LLM conversation.</p> <p>The file is represented as bytes. It does not contain any pointer to the file system.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['file']</code> <p>The type of the chunk, only for discriminated unions.</p> <code>name</code> <code>str</code> <p>The name of the file.</p> <code>mimetype</code> <code>str</code> <p>The mimetype of the file.</p> <code>bytes</code> <code>bytes</code> <p>The bytes of the file.</p> Source code in <code>src/llmir/chunks.py</code> <pre><code>class AIChunkFile(RichReprMixin, BaseModel):\n    \"\"\"\n    A file chunk in the LLM conversation.\n\n    The file is represented as bytes.\n    It does not contain any pointer to the file system.\n\n    Attributes:\n        type: The type of the chunk, only for discriminated unions.\n        name: The name of the file.\n        mimetype: The mimetype of the file.\n        bytes: The bytes of the file.\n    \"\"\"\n\n    type: Literal[\"file\"] = \"file\"\n    name: str\n    mimetype: str\n    bytes: bytes\n</code></pre>"},{"location":"api/chunks/#src.llmir.chunks.AIChunkImageURL","title":"<code>AIChunkImageURL</code>","text":"<p>               Bases: <code>RichReprMixin</code>, <code>BaseModel</code></p> <p>An image chunk in the LLM conversation.</p> <p>The image is represented as an URL that points to the image data.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['image']</code> <p>The type of the chunk, only for discriminated unions.</p> <code>url</code> <code>str</code> <p>The URL of the image.</p> Source code in <code>src/llmir/chunks.py</code> <pre><code>class AIChunkImageURL(RichReprMixin, BaseModel):\n    \"\"\"\n    An image chunk in the LLM conversation.\n\n    The image is represented as an URL that points to the image data.\n\n    Attributes:\n        type: The type of the chunk, only for discriminated unions.\n        url: The URL of the image.\n    \"\"\"\n\n    type: Literal[\"image\"] = \"image\"\n    url: str\n</code></pre>"},{"location":"api/chunks/#src.llmir.chunks.AIChunkToolCall","title":"<code>AIChunkToolCall</code>","text":"<p>               Bases: <code>RichReprMixin</code>, <code>BaseModel</code></p> <p>A tool call chunk in the LLM conversation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['tool_call']</code> <p>The type of the chunk, only for discriminated unions.</p> <code>id</code> <code>str</code> <p>The id of the tool.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>arguments</code> <code>dict[str, object]</code> <p>The arguments of the tool call. The arguments are a dictionary of key-value pairs. The values can be of any type, but should be JSON serializable. The keys are the names of the arguments.</p> Source code in <code>src/llmir/chunks.py</code> <pre><code>class AIChunkToolCall(RichReprMixin, BaseModel):\n    \"\"\"\n    A tool call chunk in the LLM conversation.\n\n    Attributes:\n        type: The type of the chunk, only for discriminated unions.\n        id: The id of the tool.\n        name: The name of the tool.\n        arguments: The arguments of the tool call. The arguments are a dictionary of key-value pairs. The values can be of any type, but should be JSON serializable. The keys are the names of the arguments.\n    \"\"\"\n\n\n    type: Literal[\"tool_call\"] = \"tool_call\"\n    id: str\n    name: str\n    arguments: dict[str, object]\n</code></pre>"},{"location":"api/messages/","title":"Messages","text":""},{"location":"api/messages/#src.llmir.messages.AIMessage","title":"<code>AIMessage</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A message in the LLM conversation.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal[USER, MODEL, SYSTEM]</code> <p>The role of the message sender.</p> <code>chunks</code> <code>list[AIChunks]</code> <p>The content chunks.</p> Source code in <code>src/llmir/messages.py</code> <pre><code>class AIMessage(BaseModel):\n    \"\"\"\n    A message in the LLM conversation.\n\n    Attributes:\n        role: The role of the message sender.\n        chunks: The content chunks.\n    \"\"\"\n\n    role: Literal[AIRoles.USER, AIRoles.MODEL, AIRoles.SYSTEM]\n    chunks: list[AIChunks] = Field(default_factory=list[AIChunks])\n</code></pre>"},{"location":"api/messages/#src.llmir.messages.AIMessageToolResponse","title":"<code>AIMessageToolResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A special tool response message in the LLM conversation.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>Literal[TOOL]</code> <p>The role of the message sender.</p> <code>chunks</code> <code>list[AIChunks]</code> <p>The content chunks.</p> <code>id</code> <code>str</code> <p>The id of the tool.</p> <code>name</code> <code>str</code> <p>The name of the tool.</p> Source code in <code>src/llmir/messages.py</code> <pre><code>class AIMessageToolResponse(BaseModel):\n    \"\"\"\n    A special tool response message in the LLM conversation.\n\n    Attributes:\n        role: The role of the message sender.\n        chunks: The content chunks.\n        id: The id of the tool.\n        name: The name of the tool.\n    \"\"\"\n\n    role: Literal[AIRoles.TOOL] = AIRoles.TOOL\n    chunks: list[AIChunks] = Field(default_factory=list[AIChunks])\n    id: str\n    name: str\n</code></pre>"},{"location":"api/rich/","title":"Rich Representation Mixin","text":""},{"location":"api/rich/#src.llmir.rich.RichReprMixin","title":"<code>RichReprMixin</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Mixin to limit the length of values in the rich representation of a Pydantic model. This is useful for large objects that should not be displayed in their entirety in the rich representation.</p> <p>Set MAX_CHARS_PER_VALUE to the maximum number of characters to display for each value. If a value exceeds this limit, it will be displayed as <code>&lt;object of length: {len(str(value))}&gt;</code>.</p> Source code in <code>src/llmir/rich.py</code> <pre><code>class RichReprMixin(BaseModel):\n    \"\"\"\n    Mixin to limit the length of values in the rich representation of a Pydantic model.\n    This is useful for large objects that should not be displayed in their entirety in the rich representation.\n\n    Set MAX_CHARS_PER_VALUE to the maximum number of characters to display for each value.\n    If a value exceeds this limit, it will be displayed as ```&lt;object of length: {len(str(value))}&gt;```.\n    \"\"\"\n\n    MAX_CHARS_PER_VALUE: int = Field(default=2000, exclude=True)\n\n    def __rich_repr__(self):\n\n        for name, field_info in self.__class__.model_fields.items():\n\n            if field_info.exclude:\n                continue\n\n            value = getattr(self, name)\n            length = len(str(value))\n\n            if length &gt; self.MAX_CHARS_PER_VALUE:\n                yield name, f\"&lt;object of length: {len(str(value))}&gt;\"\n            else:\n                yield name, value\n</code></pre>"},{"location":"api/roles/","title":"Roles","text":""},{"location":"api/roles/#src.llmir.roles.AIRoles","title":"<code>AIRoles</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Roles for the LLM messages.</p> <p>MODEL has the value assistant, because it is legacy standard to name the role of the model assistant.</p> Source code in <code>src/llmir/roles.py</code> <pre><code>class AIRoles(StrEnum):\n    \"\"\"\n    Roles for the LLM messages.\n\n    MODEL has the value assistant, because it is legacy standard to name the role of the model assistant.\n    \"\"\"\n\n    USER = \"user\"\n    MODEL = \"assistant\"\n    SYSTEM = \"system\"\n    TOOL = \"tool\"\n</code></pre>"},{"location":"api/tools/","title":"Tools","text":""},{"location":"api/tools/#src.llmir.tools.Tool","title":"<code>Tool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A tool that can be used by the LLM.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the tool.</p> <code>description</code> <code>str</code> <p>The description of the tool.</p> <code>input_schema</code> <code>dict[str, Any]</code> <p>The schema of the input of the tool. A possible schema specification can be found at the Model Context Protocol Projekt: https://github.com/modelcontextprotocol</p> Source code in <code>src/llmir/tools.py</code> <pre><code>class Tool(BaseModel):\n    \"\"\"\n    A tool that can be used by the LLM.\n\n    Attributes:\n        name: The name of the tool.\n        description: The description of the tool.\n        input_schema: The schema of the input of the tool. A possible schema specification can be found at the Model Context Protocol Projekt: https://github.com/modelcontextprotocol\n    \"\"\"\n\n    name: str\n    description: str\n    input_schema: dict[str, Any]\n</code></pre>"}]}